{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions\n",
    "spark = SparkSession.builder.appName('SparkSQL').getOrCreate()\n",
    "#spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp, to_number, cast\n",
    "from pyspark.sql.functions  import col, lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|event_datetime     |free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|0.57           |2019-03-05 08:06:14|0.51       |100      |47           |\n",
      "|0.47           |2019-03-05 08:11:14|0.62       |100      |43           |\n",
      "|0.56           |2019-03-05 08:16:14|0.57       |100      |62           |\n",
      "|0.57           |2019-03-05 08:21:14|0.56       |100      |50           |\n",
      "|0.35           |2019-03-05 08:26:14|0.46       |100      |43           |\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_utilization = spark.read \\\n",
    "    .json('utilization.json')\\\n",
    "    .withColumn('event_datetime',to_timestamp(col('event_datetime'),'MM/dd/yyyy HH:mm:ss'))\n",
    "\n",
    "df_utilization.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_utilization.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a temporary table for the dataframe in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_utilization.createOrReplaceTempView('utilization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|2019-03-05 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|2019-03-05 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|2019-03-05 08:16:14|       0.57|      100|           62|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    * \n",
    "    FROM utilization\n",
    "    LIMIT 5\n",
    "    '''\n",
    "    )\n",
    "df_sql.show(3)\n",
    "df_sql.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|cpu_util|         event_time|\n",
      "+--------+-------------------+\n",
      "|    0.57|2019-03-05 08:06:14|\n",
      "|    0.47|2019-03-05 08:11:14|\n",
      "|    0.56|2019-03-05 08:16:14|\n",
      "+--------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "        cpu_utilization as cpu_util,\n",
    "        event_datetime as event_time\n",
    "    FROM utilization\n",
    "    '''\n",
    "    )\n",
    "df_sql.show(3)\n",
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Dataframe with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.48|2019-03-09 17:21:48|       0.25|      120|           69|\n",
      "|           0.49|2019-03-15 04:36:48|       0.25|      120|           69|\n",
      "|           0.65|2019-03-10 12:01:48|       0.25|      120|           69|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16299"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    *\n",
    "    FROM utilization\n",
    "    WHERE server_id IN (100,120) AND session_count < 70\n",
    "    ORDER BY session_count DESC, free_memory ASC\n",
    "    ''' \n",
    ")\n",
    "df_sql.show(3)\n",
    "df_sql.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregrating Dataframe with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----+-------------------+------------------+\n",
      "|server_id|count|  avg|                cpu|       std_session|\n",
      "+---------+-----+-----+-------------------+------------------+\n",
      "|      103|10000|0.239| 0.7614389999999999|10.127371965435678|\n",
      "|      100|10000|0.531|  0.467506000000003|10.198979446894468|\n",
      "|      101|10000|0.203| 0.7985559999999872|10.044359210780733|\n",
      "|      102|10000|0.244| 0.7583949999999904|10.133512315440532|\n",
      "|      107|10000|0.349| 0.6505060000000022|10.143694072819049|\n",
      "|      104|10000|0.285| 0.7108530000000015|10.086652928337022|\n",
      "|      106|10000|0.575| 0.4220220000000024|10.163578156441464|\n",
      "|      105|10000|0.509|0.49256400000000206| 10.12206970631569|\n",
      "|      110|10000|0.444| 0.5537749999999991|10.114235813278661|\n",
      "|      108|10000|0.255| 0.7476360000000036|10.085062512874618|\n",
      "|      109|10000|0.438| 0.5630629999999914|10.089432276613316|\n",
      "|      112|10000|0.286| 0.7153870000000067|10.116853899967271|\n",
      "|      113|10000|0.216| 0.7833319999999914|10.122367206922872|\n",
      "|      114|10000|0.472| 0.5294289999999995|10.079504247758821|\n",
      "|      111|10000|0.439| 0.5592459999999951|  10.1669565977556|\n",
      "|      116|10000|0.503| 0.4970290000000043|10.073724385365114|\n",
      "|      115|10000| 0.36| 0.6398790000000023|  10.0715118192371|\n",
      "|      117|10000|0.417|  0.582722999999997| 10.03435800009386|\n",
      "|      119|10000|0.582| 0.4194930000000031|10.087034320658292|\n",
      "|      120|10000|0.447| 0.5520779999999899|  9.99763865309521|\n",
      "+---------+-----+-----+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql = spark.sql(\n",
    "'''\n",
    "    SELECT \n",
    "        server_id,\n",
    "        count(*)  count,\n",
    "        round(avg(free_memory),3) avg,\n",
    "        avg(cpu_utilization) cpu,\n",
    "        std(session_count) std_session\n",
    "    FROM utilization\n",
    "    GROUP BY server_id\n",
    "    ORDER BY count(*) DESC\n",
    "''' \n",
    ")\n",
    "df_sql.show()\n",
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Dataframes with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- server_id: string (nullable = true)\n",
      " |-- server_name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- cpu_utilization: double (nullable = true)\n",
      " |-- event_datetime: timestamp (nullable = true)\n",
      " |-- free_memory: double (nullable = true)\n",
      " |-- server_id: long (nullable = true)\n",
      " |-- session_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('server_name.csv', header=True).printSchema()\n",
    "df_utilization.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svrs = spark.read.csv('server_name.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svrs.createOrReplaceTempView('servers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_ROUTINE] Cannot resolve function `convert` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`].; line 7 pos 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;43;03m'''\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;43;03m    SELECT \u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;43;03m        u.*,\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;43;03m        v.*\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;43;03m    FROM utilizations u\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;43;03m    INNER JOIN servers v\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;43;03m    ON convert(u.server_id , VARCHAR) = v.server_id\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;43;03m'''\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[0;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[0;32m   1630\u001b[0m         )\n\u001b[1;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_ROUTINE] Cannot resolve function `convert` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`].; line 7 pos 7"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "'''\n",
    "    SELECT \n",
    "        u.*,\n",
    "        v.*\n",
    "    FROM utilizations u\n",
    "    INNER JOIN servers v\n",
    "    ON convert(u.server_id , VARCHAR) = v.server_id\n",
    "'''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-data-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
